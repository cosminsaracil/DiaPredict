Cell 1: Environment Setup and Data Loading

    Setting up the environment. It imports essential Python libraries required for data analysis, manipulation, and visualization, ensuring that all necessary tools are available for subsequent steps in your pipeline.
    Verifying the environment. It prints details about the Python environment, such as the executable path, version, and site packages, which helps confirm that the correct environment is being used—a useful check for reproducibility or debugging.
    Loading the dataset: It specifies the path to a CSV file containing diabetes-related data, checks if the file exists, and loads it into a pandas DataFrame for further processing. It also provides a quick preview of the data by displaying column names and the first few rows.

    Important note. The data is loaded locally because of the nature of the dataset. If you plan to use this code make sure to make a copy of the dataset available on Kaggle.

Cell 4: Specificity of the values

    It counts the number of unique values per column and presents the results in a neat, readable format.

Cell 5: Distribution of data

    Designed to check the frequency of values in various columns of your (df). This cell is a key part of Exploratory Data Analysis (EDA), helping you visualize how data is distributed across different features.
    Generates a grid of histograms, one for each feature in your (df) (except 'stroke'). A histogram shows how often different values or ranges of values appear in a column. For example:

    If a feature like “age” has a histogram with a peak at 40, it means many people in your dataset are around 40 years old.
    If a categorical feature like “gender” has two bars (e.g., for “Male” and “Female”), it shows how many entries fall into each category.

Cell 6: Correlation between any features

    The cell aims to visualize how each feature in your dataset relates to the target variable, 'Diabetes'.
    By default, it uses Pearson correlation, which measures the strength and direction of the linear relationship between two variables. The output is a Series where each index is a column name from df, and each value is the correlation coefficient (ranging from -1 to 1) with 'Diabetes'.

    Features with high positive (close to 1) or negative (close to -1) correlations are likely more relevant for predicting 'Diabetes'.
    Features with correlations near zero may have little linear relationship with the target and could potentially be dropped to simplify your model.

    Positive correlations mean that as the feature value increases, 'Diabetes' tends to increase (e.g., higher age might correlate with higher diabetes risk).
    Negative correlations suggest the opposite (e.g., higher physical activity might reduce diabetes likelihood).

    While this plot focuses on the target, it’s a reminder to check for multicollinearity (high correlations between features) later, as it can affect models like linear regression.

Cell 7: Heatmap

    Creates a correlation heatmap for df.
    cmap='coolwarm': Applies the “coolwarm” color scheme—blue for negative correlations, red for positive ones, and neutral colors near zero.
    Rows and columns correspond to the features (columns) in df.
    Each cell’s color and number represent the correlation between the row and column features.

    Spotting multicollinearity:

    High correlations (e.g., above 0.8 or below -0.8) between features suggest redundancy. This can affect machine learning models like linear regression by causing overfitting or unstable predictions.

    Understanding relationships:

    It reveals how features relate to each other and to the target variable (if included in df). For example, in a diabetes dataset, you might see BMI and weight strongly correlated, which aligns with real-world expectations.

    Reading: 
        Diagonal: Cells where a feature correlates with itself (e.g., “Age” vs. “Age”) will show 1 (bright red), since they’re perfectly correlated.
        Target Correlations: “Diabetes” is the target so its row or column shows how each feature correlates with it—useful for identifying key predictors.
        Feature Pairs: Off-diagonal cells show relationships between different features. 
        For example:
        “Age” vs. “BMI” = 0.6 (red): As age increases, BMI tends to increase.
        “Exercise” vs. “BMI” = -0.4 (blue): More exercise is associated with lower BMI.

            Color Guide:

            Red: Positive correlation (features move together).
            Blue: Negative correlation (features move oppositely).
            White/Pale: Little to no correlation.

Cell 11: Data splitting
    
    Prepares the dataset for training and evaluating a machine learning model by splitting it into training and testing sets.
    Defining the target variable (y)
        y = (df['Diabetes']).astype(int): This extracts the 'Diabetes' column from (df) and converts it to integers. This column is your target variable—what you aim to predict (e.g., 0 for no diabetes, 1 for diabetes).
    Defining the Feature Variables (X)
        X = df.loc[:, df.columns != 'Diabetes']: This selects all columns in df except 'Diabetes'. These are your features—the inputs your model will use to make predictions.

    X_train, y_train: Training features and target (80% of the data).
    X_test, y_test: Testing features and target (20% of the data).

Cell 12 : Hyperparameter tunning 

    Hyperparameter Tuning: Testing different combinations of parameters to improve results.
    K-Folds Cross-Validation: The cv=5 in GridSearchCV implements 5-fold cross-validation. 
    Random Seeding: random_state=42 ensures reproducibility, aligning with the need for random seeding in splits.

Cell 13: Ablation testing

    Purpose: This cell performs ablation testing by varying the max_depth parameter of a Decision Tree Classifier (1, 3, 5, 7, unlimited) to see how tree depth impacts performance.
    Process: For each depth, it trains a new model on X_train and y_train, predicts on X_test, computes accuracy, and stores/results are printed.
    Output: Accuracy scores for each depth, e.g., Max Depth: 3, Test Accuracy: 0.7200.
    This cell isolates max_depth and tests it systematically (including 1 and None) on the test set (X_test), not cross-validation. It’s ablation testing, focusing on understanding the specific effect of depth alone, holding other parameters constant (criterion='gini', defaults for others).

    NOTE The variable clf is redefined in each loop iteration with a new DecisionTreeClassifier instance. After the loop ends, clf holds the last model (depth=None), but this doesn’t affect clf_gini from your previous hyperparameter tuning cell (where clf_gini was set to the best model with max_depth=7).
    NOTE Key Point: This cell doesn’t update clf_gini—it’s just testing different depths and printing results. The main model (clf_gini) remains unchanged unless you explicitly reassign it.

Cell 14: Scalability testing

    Purpose: Performs scalability testing by measuring how training time varies with dataset size and tree depth.
    Process: 
        Subsamples X_train and y_train at fractions (25%, 50%, 75%, 100%).
        Tests tree depths (3, 5, 10).
        Times the fit operation for each combination and prints the results.
    Output: Shows training time (in seconds) for each size-depth pair, e.g., Data Fraction: 0.5, Depth: 5, Time: 0.0123 seconds.

    Scalability Testing: Fully satisfies this requirement by computing execution time with respect to dataset size (simulating batch size) and tree depth (model complexity). Decision trees don’t use epochs or convergence in the traditional sense, so this adapts well to the algorithm.

    Small Data, Shallow Tree: Fastest (0.0121s at 25%, Depth 3).
    Full Data, Deep Tree: Slowest (0.1502s at 100%, Depth 10)—~12x slower than the fastest.
    Pattern: Time grows roughly linearly with data size and slightly more with depth due to increased complexity.

    Key Insights

    Scalability: Your Decision Tree scales reasonably—training is fast (<0.2s) even with full data and depth 10, typical for small-to-medium datasets.
    Trade-Off: Deeper trees (e.g., 7, your tuned depth) take longer but might improve accuracy (from ablation). Balance time vs. performance.
    Practicality: Times are low, so scalability isn’t a bottleneck here, but this confirms deeper models and more data demand more computation.
    Confirm 7 depth.

Cell 15 : K-Folds Cross-Validation

    Purpose: Implements stratified k-folds cross-validation with 10 runs to evaluate the tuned Decision Tree model, computing multiple metrics and their mean/std.
    Process:

        Loops over 10 seeds (different random splits).
        For each seed, performs 5-fold stratified cross-validation on X and y.
        Within each fold, splits the training data into 90% train and 10% validation.
        Trains on the 90% subset (X_train_sub, y_train_sub) and predicts on the test fold (X_test_fold).
        Collects accuracy, precision, recall, and F1-score, then computes mean and std.

    Output: Prints mean and standard deviation for each metric across 50 evaluations (10 runs × 5 folds).

    Accuracy

        Definition: The proportion of correct predictions (both true positives and true negatives) out of all predictions.
        Formula: (TP + TN) / (TP + TN + FP + FN), where TP = true positives, TN = true negatives, FP = false positives, FN = false negatives.
        Output: Accuracy - Mean: 0.7405, Std: 0.0037
            Mean (0.7405): On average, your model correctly predicts diabetes status 74.05% of the time across 50 evaluations (10 runs × 5 folds).
            Std (0.0037): The accuracy varies by only ±0.37% across runs, indicating high stability.
    Precision

        Definition: The proportion of predicted positives (1s) that are actually positive. It measures how reliable a positive prediction is.
        Formula: TP / (TP + FP).
        Your Output: Precision - Mean: 0.7222, Std: 0.0088
            Mean (0.7222): When your model predicts diabetes (1), it’s correct 72.22% of the time. About 28% of positive predictions are false positives.
            Std (0.0088): Precision varies by ±0.88%, showing consistent performance.
    Recall (Sensitivity)

        Definition: The proportion of actual positives (1s) that are correctly predicted. It measures how well the model catches all positive cases.
        Formula: TP / (TP + FN).
        Your Output: Recall - Mean: 0.7825, Std: 0.0171
            Mean (0.7825): The model identifies 78.25% of actual diabetes cases, missing about 22% (false negatives).
            Std (0.0171): Recall fluctuates by ±1.71%, suggesting slightly more variability than other metrics.

    F1-Score

        Definition: The harmonic mean of precision and recall, balancing the trade-off between them. It’s useful when classes are imbalanced.
        Formula: 2 * (Precision * Recall) / (Precision + Recall).
        Your Output: F1 - Mean: 0.7509, Std: 0.0047
            Mean (0.7509): An F1 of 75.09% reflects a good balance between precision (72.22%) and recall (78.25%).
            Std (0.0047): F1 varies by ±0.47%, indicating stable performance.